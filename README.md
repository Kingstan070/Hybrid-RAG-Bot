# ğŸš€ Hybrid RAG Support Bot

### **Structure-Aware Retrieval-Augmented Generation for PDFs**

_A Local, Privacy-Preserving RAG System Designed for Real-World Documents_

---

## ğŸ“Œ Introduction

This project aims to overcome a major limitation in most Retrieval-Augmented Generation (RAG) systems:

> **RAG systems do not understand document structure â€” they only process flat text.**

PDFs in the real world contain:
- Different formatting styles
- Missing/irregular table of contents
- Long paragraphs without headings
- Page-level context where meaning depends on structure

**Our goal** was to build a robust **structure-aware RAG pipeline** that understands PDFs using:  
âœ” TOC detection (Level-1 / Level-2 / No TOC)  
âœ” Chapter + section metadata extraction  
âœ” Paragraph-based chunking  
âœ” Keyword extraction using RAKE  
âœ” Local embeddings using Ollama  
âœ” Hybrid retrieval â€” metadata + similarity search  
âœ” Latency measurement for each stage

**What is achieved now:**  
âœ” PDF parsing with structural awareness  
âœ” Metadata + keyword-rich embeddings stored in ChromaDB  
âœ” Working CLI retrieval  
âœ” A functional API (`api/app.py`) for querying via FastAPI  
âœ” Fully local execution â€” **no cloud APIs used**

**This is a working prototype, tested on CPU, built within a limited time.**  
More testing and improvements are planned in future updates.

---

## ğŸ§  Core Modules Used (and WHY)

|Module|Purpose|
|---|---|
|**PyMuPDF (`fitz`)**|PDF parsing, page extraction, TOC reading|
|**RAKE-NLTK**|Keyword extraction for boosting retrieval relevance|
|**LangChain**|Framework to connect vector DB + LLM + retriever|
|**ChromaDB**|Local vector database to store embeddings & metadata|
|**Ollama**|Local LLM inference â€” no API key, fully offline|
|**FastAPI**|API layer to interact with the RAG system|
|**Pydantic**|Configuration management (`config/settings.py`)|
|**Custom Loggers**|Tracks parsing, embeddings, query latency|
|**NumPy / Pandas**|/Data handling for chunks & processed CSVs|

---

## âš™ï¸ Setup Instructions

### ğŸ§ª 1ï¸âƒ£ Create Conda Environment

```bash
conda env create -f environment.yml
conda activate hybrid_bot
```

### ğŸ“¦ 2ï¸âƒ£ Install Python Dependencies

```bash
pip install -r requirements.txt
```

---

### ğŸ¤– 3ï¸âƒ£ Install Ollama (Required)

Download & install from:  
ğŸ”— [https://ollama.ai/download](https://ollama.ai/download)

### ğŸ“¥ 4ï¸âƒ£ Pull Models (Embedding + LLM)

```bash
ollama pull mxbai-embed-large     # Embedding model
ollama pull llama3.2:3b           # LLM for generation
```

---

## â–¶ Usage â€” How the System Works

### ğŸ“Œ Why Do We Ingest the PDF?

Before retrieval is possible, we must:  
âœ” Parse PDF + detect structure  
âœ” Split into meaningful chunks  
âœ” Extract metadata & keywords  
âœ” Store in CSV & JSON formats

### ğŸ“Œ Why Do We Build ChromaDB?

Once chunks are extracted, we:  
âœ” Embed them using `mxbai-embed-large`  
âœ” Store them inside ChromaDB  
âœ” Enable retrieval using LangChain Retriever

---

### ğŸš€ **Run These Scripts (In Correct Order)**

| Step | Command                                                       | Purpose                       |
| ---- | ------------------------------------------------------------- | ----------------------------- |
| 1ï¸âƒ£  | `python scripts/ingest_pdf.py --pdf_path input.pdf --chunk`   | Parse PDF & generate metadata |
| 2ï¸âƒ£  | `python scripts/build_chroma.py`                              | Build embeddings + local DB   |

âš  **Note:** `query_chroma.py` is only for testing.  

---

## ğŸ“ Project Structure

```
Hybrid-RAG-Bot/
â”‚
â”œâ”€â”€ ingestion/              â† PDF parsing + TOC detection + chunking
â”œâ”€â”€ embeddings/             â† Embedding + ChromaDB builder
â”œâ”€â”€ rag/                    â† Retrieval + LLM pipeline
â”œâ”€â”€ api/                    â† FastAPI interface (basic)
â”œâ”€â”€ scripts/                â† RUN THESE FIRST (pipeline scripts)
â”œâ”€â”€ app_logging/            â† Modular logging system
â”œâ”€â”€ config/settings.py      â† Central configuration
â”œâ”€â”€ data/                   â† Output CSVs + vector DB
â””â”€â”€ README.md
```

(Verified via `project_snapshot.txt`)

---

## âš  Known Issues / Limitations

| Issue                         | Reason                                        |
| ----------------------------- | --------------------------------------------- |
| Limited testing               | Developed only on CPU (time-limited)          |
| Threshold values              | Tuned manually ("eyeballing") â†’ needs testing |
| Only one API endpoint exists  | Due to project deadline                       |
| CLI retrieval logs incomplete | API-based logging recommended                 |

---

## ğŸ”§ Future Fixes & Improvements

ğŸš€ Planned features:

- Add more API endpoints (upload PDF, rebuild DB, test queries)
- Automate ingestion & embedding â€” **no CLI required**
- Improve PDF generalization across formats
- Add Streamlit UI for user-friendly front-end
- Confidence scoring + metadata filtering
- Load balancing for large PDFs

---

## ğŸ“¬ Contact

**Author:** Allwin Kingstan  
ğŸ“§ **[tallwinkingstan@gmail.com](mailto:tallwinkingstan@gmail.com)**  

ğŸ”— GitHub: `https://github.com/Kingstan070`
